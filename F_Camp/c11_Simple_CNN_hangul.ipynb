{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 11. Simple CNN : 한글 자모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* transforms에 대해서는 다음 참조\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
      "{'ㄱ': 0, 'ㄲ': 1, 'ㄴ': 2, 'ㄷ': 3, 'ㄸ': 4, 'ㄹ': 5, 'ㅁ': 6, 'ㅂ': 7, 'ㅃ': 8, 'ㅅ': 9, 'ㅆ': 10, 'ㅇ': 11, 'ㅈ': 12, 'ㅉ': 13, 'ㅊ': 14, 'ㅋ': 15, 'ㅌ': 16, 'ㅍ': 17, 'ㅎ': 18, 'ㅏ': 19, 'ㅐ': 20, 'ㅑ': 21, 'ㅒ': 22, 'ㅓ': 23, 'ㅔ': 24, 'ㅕ': 25, 'ㅖ': 26, 'ㅗ': 27, 'ㅘ': 28, 'ㅙ': 29, 'ㅛ': 30, 'ㅜ': 31, 'ㅝ': 32, 'ㅞ': 33, 'ㅟ': 34, 'ㅠ': 35, 'ㅡ': 36, 'ㅢ': 37, 'ㅣ': 38}\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"data/hangul/\"\n",
    "img_data = dsets.ImageFolder(img_dir, \n",
    "                             transforms.Compose([\n",
    "                                 transforms.Grayscale(),\n",
    "                                 \n",
    "#                                  # Data Augmentation\n",
    "#                                  transforms.RandomRotation(15)\n",
    "#                                  transforms.CenterCrop(28),\n",
    "#                                  transforms.Lambda(lambda x: x.rotate(15)),\n",
    "                                 \n",
    "#                                  # Data Nomalization\n",
    "#                                  transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "\n",
    "                                 transforms.ToTensor(),\n",
    "                             ]))\n",
    "\n",
    "print(img_data.classes)\n",
    "print(img_data.class_to_idx)  # class 39 - 각 class별 720개 이미지 존재."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28080"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_data) # = 39 * 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/hangul/ㄱ\\\\001.png', 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data.imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x200dd51f7b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAC69JREFUeJzt3V2oZfV5x/Hvr1bb0gR08KXiS8eG\nIVVKHWEqA+mFNbVMvdFAUiI0zIVgChESCKU2N0lLAwaa2FyEgKnWKaRRiUkdin2RiSUNFOvEWDPJ\npGjsNJk4zIxEid4kjD692OvA6fScmT1nv5y9z/P9wGbvtfZaZz//mfM7a++111pPqgpJ/fzcZhcg\naXMYfqkpwy81Zfilpgy/1JThl5oy/FJThl9qyvBLTf38JCsn2QN8FjgP+OuquvdMy1988cW1ffv2\nSV5S0hkcOXKEV155JeMsu+HwJzkP+BxwC3AUeCbJ/qr67nrrbN++nYMHD270JSWdxa5du8ZedpK3\n/TcCL1bVS1X1M+Bh4LYJfp6kOZok/FcAP1w1fXSY938kuSvJwSQHT548OcHLSZqmScK/1ueK/3eK\nYFXdX1W7qmrXJZdcMsHLSZqmScJ/FLhq1fSVwMuTlSNpXiYJ/zPAjiTXJLkAeD+wfzplSZq1De/t\nr6pTSe4G/pnRV30PVtV3plaZpJma6Hv+qnoCeGJKtUiaI4/wk5oy/FJThl9qyvBLTRl+qSnDLzVl\n+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pKcMvNWX4paYMv9SU4ZeaMvxSU4ZfasrwS00ZfqmpSbv0\nHgFeB94ETlXV+F0CJW2qicI/+J2qemUKP0fSHPm2X2pq0vAX8C9JvpnkrmkUJGk+Jn3b/66qejnJ\npcCTSb5XVV9fvcDwR+EugKuvvnrCl5M0LRNt+avq5eH+BPBV4MY1lrFFt7SANhz+JL+c5O0rj4Hf\nAw5NqzBJszXJ2/7LgK8mWfk5f1dV/zSVqiTN3CQtul8Crp9iLZLmyK/6pKYMv9SU4ZeaMvxSU4Zf\nasrwS00Zfqkpwy81Zfilpgy/1JThl5oy/FJThl9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmjL8UlOG\nX2rqrOFP8mCSE0kOrZq3LcmTSV4Y7i+abZmSpm2cLf9DwJ7T5t0DHKiqHcCBYVrSEjlr+Ifeez8+\nbfZtwL7h8T7g9inXJWnGNvqZ/7KqOgYw3F86vZIkzcPMd/gluSvJwSQHT548OeuXkzSmjYb/eJLL\nAYb7E+staJdeaTFtNPz7gb3D473A49MpR9K8jPNV35eAfwfemeRokjuBe4FbkrwA3DJMS1oiZ+3S\nW1V3rPPUu6dci6Q58gg/qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pKcMvNWX4paYMv9SU\n4ZeaMvxSU4ZfasrwS00Zfqkpwy81Zfilpgy/1JThl5raaJfeTyT5UZLnhtutsy1T0rRttEsvwH1V\ntXO4PTHdsiTN2ka79EpacpN85r87yfPDx4KLplaRpLnYaPg/D7wD2AkcAz693oJ26ZUW04bCX1XH\nq+rNqnoL+AJw4xmWtUuvtIA2FP6V9tyD9wCH1ltW0mI6a6POoUvvTcDFSY4CHwduSrITKOAI8MEZ\n1ihpBjbapfeBGdQiaY48wk9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNGX6pKcMv\nNWX4paYMv9SU4ZeaMvxSU4ZfasrwS00Zfqkpwy81Zfilpgy/1NQ41+2/Cvhb4FeAt4D7q+qzSbYB\njwDbGV27/w+q6tXZlaq1JNnsEsZSVZtdgk4zzpb/FPDRqroW2A18KMl1wD3AgaraARwYpiUtiXFa\ndB+rqmeHx68Dh4ErgNuAfcNi+4DbZ1WkpOk7p8/8SbYDNwBPA5dV1TEY/YEALl1nHbv0Sgto7PAn\neRvwGPCRqvrJuOvZpVdaTGOFP8n5jIL/xar6yjD7+Eq33uH+xGxKlDQLZw1/RruTHwAOV9VnVj21\nH9g7PN4LPD798vpJck63ZbFVx7XMzvpVH/Au4APAt5M8N8z7GHAv8GiSO4EfAO+bTYmSZmGcFt3f\nANb7U/zu6ZYjaV48wk9qyvBLTRl+qalxdvhpBqa1R3vRjpl3T/3ycMsvNWX4paYMv9SU4ZeaMvxS\nU+7tXxKLtld/PevVea7fAqy3/LL8OywDt/xSU4ZfasrwS00Zfqkpwy815d7+JdFt7/dWHdciccsv\nNWX4paYMv9SU4ZeaMvxSU+Nct/+qJE8lOZzkO0k+PMz/RJIfJXluuN06+3K3jqpa83auzvV6+Jt1\n0+IZ56u+lS69zyZ5O/DNJE8Oz91XVX85u/Ikzco41+0/Bqw05Hw9yUqXXklLbJIuvQB3J3k+yYNJ\nLlpnHbv0Sgtoki69nwfeAexk9M7g02utZ5deaTFtuEtvVR2vqjer6i3gC8CNsytT0rSd9TP/el16\nk1w+7A8AeA9waDYl9nKue/yXZU+6x+ovnkm69N6RZCdQwBHggzOpUNJMTNKl94nplyNpXjzCT2rK\n8EtNGX6pKa/ks+Tci66NcssvNWX4paYMv9SU4ZeaMvxSU4ZfasrwS00Zfqkpwy81Zfilpgy/1JTh\nl5oy/FJThl9qyvBLTRl+qSnDLzU1TpfeX0zyH0n+c+jS+2fD/GuSPJ3khSSPJLlg9uVKmpZxtvw/\nBW6uqusZtebak2Q38ClGXXp3AK8Cd86uTEnTdtbw18gbw+T5w62Am4EvD/P3AbfPpEJJMzFur77z\nhm49J4Ange8Dr1XVqWGRo9i2W1oqY4V/aMi5E7iSUUPOa9dabK11bdEtLaZz2ttfVa8B/wrsBi5M\nsnLp7yuBl9dZxxbd0gIaZ2//JUkuHB7/EvC7wGHgKeC9w2J7gcdnVaSk6RunacflwL4k5zH6Y/Fo\nVf1Dku8CDyf5C+BbjNp4S1oS43TpfR64YY35LzH6/C9pCXmEn9SU4ZeaMvxSU4ZfasrwS00Zfqkp\nwy81Zfilpgy/1JThl5oy/FJThl9qyvBLTRl+qSnDLzVl+KWmDL/UlOGXmjL8UlOGX2rK8EtNTdKl\n96Ek/53kueG2c/blSpqWca7bv9Kl940k5wPfSPKPw3N/XFVfPsO6khbUONftL2CtLr2SltiGuvRW\n1dPDU59M8nyS+5L8wsyqlDR1G+rSm+Q3gD8Ffh34LWAb8CdrrWuXXmkxbbRL756qOlYjPwX+hnVa\nd9mlV1pMG+3S+70klw/zAtwOHJploZKmK6P9eWdYIPlNYB+wukvvnyf5GnAJEOA54I+q6o31fxIk\nOQn8zzB5MfDKZOUvlW7jhX5jXoTx/mpVjfUW+6zhn5UkB6tq16a8+CboNl7oN+ZlG69H+ElNGX6p\nqc0M//2b+Nqbodt4od+Yl2q8m/aZX9Lm8m2/1NTcw59kT5L/SvJiknvm/frzkOTBJCeSHFo1b1uS\nJ5O8MNxftJk1TlOSq5I8leTwcObnh4f5W3nM653tek2Sp4cxP5Lkgs2udT1zDX+S84DPAb8PXAfc\nkeS6edYwJw8Be06bdw9woKp2AAeG6a3iFPDRqroW2A18aPh/3cpjXjnb9XpgJ7AnyW7gU8B9w5hf\nBe7cxBrPaN5b/huBF6vqpar6GfAwcNuca5i5qvo68OPTZt/G6GAphvvb51rUDA2Hej87PH4dOAxc\nwdYec606qG312a43AyunuS/0mOcd/iuAH66aPjrM6+CyqjoGo7AAl25yPTORZDtwA/A0W3zMp5/t\nCnwfeK2qTg2LLPTv97zDnzXm+XXDFpHkbcBjwEeq6iebXc+snX62K3DtWovNt6rxzTv8R4GrVk1f\nCbw85xo2y/FVJ0NdzmhrsWUMV3l6DPhiVX1lmL2lx7xi1dmuu4ELk6xcJGehf7/nHf5ngB3DHtEL\ngPcD++dcw2bZD+wdHu8FHt/EWqZqOLPzAeBwVX1m1VNbecxrne16GHgKeO+w2EKPee4H+SS5Ffgr\nRmcJPlhVn5xrAXOQ5EvATYzO8joOfBz4e+BR4GrgB8D7qur0nYJLKclvA/8GfBt4a5j9MUaf+7fq\nmNc72/XXGO3I3gZ8C/jD4ZoXC8cj/KSmPMJPasrwS00Zfqkpwy81Zfilpgy/1JThl5oy/FJT/wse\nsHpvPi1kLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.open(\"data/hangul/ㅇ/111.png\").convert(\"L\")   # 36 * 36 image\n",
    "imgarr = np.array(img)\n",
    "print(imgarr.shape)\n",
    "plt.imshow(imgarr, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "font_num = 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, train_ratio, stratify, stratify_num, batch_size) :\n",
    "    \n",
    "    length = len(data)\n",
    "    \n",
    "    # 층화 추출\n",
    "    if stratify :\n",
    "        label_num = int(len(data)/stratify_num)\n",
    "        cut = int(stratify_num*train_ratio)\n",
    "        train_indices = np.random.permutation(np.arange(stratify_num))[:cut]\n",
    "        test_indices = np.random.permutation(np.arange(stratify_num))[cut:]\n",
    "        \n",
    "        for i in range(1, label_num) :\n",
    "            train_indices = np.concatenate((train_indices, np.random.permutation(np.arange(stratify_num))[:cut] + stratify_num*i))\n",
    "            test_indices = np.concatenate((test_indices, np.random.permutation(np.arange(stratify_num))[cut:] + stratify_num*i))\n",
    "        \n",
    "    else :\n",
    "        cut = int(len(data)*train_ratio)\n",
    "        train_indices = np.random.permutation(np.arange(length))[:cut]\n",
    "        test_indices = np.random.permutation(np.arange(length))[cut:]\n",
    "        \n",
    "    sampler = Data.SubsetRandomSampler(train_indices)\n",
    "    \n",
    "    train_loader = Data.DataLoader(data, batch_size=batch_size, shuffle=False, sampler=sampler, num_workers=0, drop_last=True)\n",
    "    test_loader = Data.DataLoader(data, batch_size=batch_size, shuffle=False, sampler=sampler, num_workers=0, drop_last=True)\n",
    "\n",
    "    return train_loader, test_loader, len(train_indices), len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_num, test_num = train_test_split(img_data, 0.8, True, font_num, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22464, 5616)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num, test_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def c_conv(N, K, P=0, S=1):\n",
    "    return int((N + 2*P - K) / S + 1)\n",
    "\n",
    "def c_pool(N, K):\n",
    "    return int(N/K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 32 16 14 12 6\n"
     ]
    }
   ],
   "source": [
    "c0 = 36\n",
    "c1 = c_conv(c0, 3)\n",
    "c2 = c_conv(c1, 3)\n",
    "c3 = c_pool(c2, 2)\n",
    "\n",
    "c4 = c_conv(c3, 3)\n",
    "c5 = c_conv(c4, 3)\n",
    "c6 = c_pool(c5, 2)\n",
    "\n",
    "print(c1, c2, c3, c4, c5, c6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Test 1 - 84.78 %\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3),    # 36 --> 34\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16,32,3),   # 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),    # 16\n",
    "\n",
    "            nn.Conv2d(32,64,3),   # 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64,128,3),   # 12\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)      # 6\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(128*6*6,300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,39)\n",
    "        )\n",
    "        \n",
    "        # Weight Initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # init.xavier_normal(m.weight.data)\n",
    "                init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "model = CNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Adam\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Momentum & Weight Regularization(L2)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma= 0.99)\n",
    "# scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10,30,80], gamma= 0.1)\n",
    "# scheduler = lr_scheduler.ExponentialLR(optimizer, gamma= 0.99)\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], lter [224/224] Loss: 0.28873\n",
      "Epoch [2/10], lter [224/224] Loss: 0.13322\n",
      "Epoch [3/10], lter [224/224] Loss: 0.05497\n",
      "Epoch [4/10], lter [224/224] Loss: 0.02143\n",
      "Epoch [5/10], lter [224/224] Loss: 0.02459\n",
      "Epoch [6/10], lter [224/224] Loss: 0.03388\n",
      "Epoch [7/10], lter [224/224] Loss: 0.01384\n",
      "Epoch [8/10], lter [224/224] Loss: 0.01262\n",
      "Epoch [9/10], lter [224/224] Loss: 0.00434\n",
      "Epoch [10/10], lter [224/224] Loss: 0.01707\n"
     ]
    }
   ],
   "source": [
    "total_batch = train_num//batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "#     scheduler.step()\n",
    "\n",
    "    for i, (batch_images, batch_labels) in enumerate(train_loader):\n",
    "\n",
    "        X = batch_images.cuda()\n",
    "        Y = batch_labels.cuda()\n",
    "\n",
    "        pred = model(X)\n",
    "        cost = loss(pred, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) == total_batch:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.5f'%(epoch+1, num_epochs, i+1, total_batch, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'cnn_hangul_Adam.pkl')\n",
    "# print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 22398\n",
      "total   : 22400\n",
      "Accuracy of test images: 99.991071\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    \n",
    "    images = images.cuda()\n",
    "    outputs = model(images)\n",
    "    \n",
    "#     print(outputs.data)   # 39 class에 대한 확률\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.cuda()).sum()\n",
    "    \n",
    "    \n",
    "correct = correct.cpu().numpy()    \n",
    "print('correct :', correct)\n",
    "print('total   :', total)\n",
    "print('Accuracy of test images: %f' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
