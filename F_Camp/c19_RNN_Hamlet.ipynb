{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# many to many 구조 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162881 \n",
      "\n",
      "[The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo and Francisco two Centinels.\n",
      "\n",
      "  Barnardo. Who's there?\n",
      "  Fran. Nay answer me: Stand & vnfold\n",
      "your selfe\n",
      "\n",
      "   Bar. Long liue the King\n",
      "\n",
      "   Fran. Barnardo?\n",
      "  Bar. He\n",
      "\n",
      "   Fran. You come most carefully vpon your houre\n",
      "\n",
      "   Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
      "\n",
      "   Fran. For this releefe much thankes: 'Tis bitter cold,\n",
      "And I am sicke at heart\n",
      "\n",
      "   Barn. Haue you had quiet Guard?\n",
      "  Fran. Not\n"
     ]
    }
   ],
   "source": [
    "raw = nltk.corpus.gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
    "print(len(raw), '\\n')\n",
    "print(raw[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char to Dic\n",
    "\n",
    "word가 아닌 character 단위 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2index = {}\n",
    "index2char = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for char in raw :\n",
    "    if char not in char2index.keys() :\n",
    "        char2index[char] = len(char2index)\n",
    "        index2char.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 27,\n",
       " ' ': 4,\n",
       " '!': 64,\n",
       " '&': 43,\n",
       " \"'\": 39,\n",
       " '(': 61,\n",
       " ')': 62,\n",
       " ',': 48,\n",
       " '-': 54,\n",
       " '.': 32,\n",
       " '1': 23,\n",
       " '5': 24,\n",
       " '9': 25,\n",
       " ':': 42,\n",
       " ';': 58,\n",
       " '?': 40,\n",
       " 'A': 28,\n",
       " 'B': 35,\n",
       " 'C': 38,\n",
       " 'D': 55,\n",
       " 'E': 34,\n",
       " 'F': 36,\n",
       " 'G': 51,\n",
       " 'H': 12,\n",
       " 'I': 49,\n",
       " 'K': 46,\n",
       " 'L': 45,\n",
       " 'M': 52,\n",
       " 'N': 41,\n",
       " 'O': 56,\n",
       " 'P': 31,\n",
       " 'Q': 59,\n",
       " 'R': 53,\n",
       " 'S': 19,\n",
       " 'T': 1,\n",
       " 'V': 63,\n",
       " 'W': 18,\n",
       " 'Y': 47,\n",
       " 'Z': 66,\n",
       " '[': 0,\n",
       " ']': 26,\n",
       " 'a': 6,\n",
       " 'b': 16,\n",
       " 'c': 29,\n",
       " 'd': 8,\n",
       " 'e': 3,\n",
       " 'f': 11,\n",
       " 'g': 7,\n",
       " 'h': 2,\n",
       " 'i': 9,\n",
       " 'j': 65,\n",
       " 'k': 20,\n",
       " 'l': 14,\n",
       " 'm': 13,\n",
       " 'n': 33,\n",
       " 'o': 10,\n",
       " 'p': 22,\n",
       " 'q': 50,\n",
       " 'r': 5,\n",
       " 's': 21,\n",
       " 't': 15,\n",
       " 'u': 30,\n",
       " 'v': 44,\n",
       " 'w': 37,\n",
       " 'x': 57,\n",
       " 'y': 17,\n",
       " 'z': 60}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2vec = {}\n",
    "eye = np.eye(len(char2index))   # identity matrix (대각행렬) --> one hot encoding\n",
    "\n",
    "for item in char2index.keys() :\n",
    "    char2vec[item] = eye[char2index[item],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2vec['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2vec['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162881, 67)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text 문서의 전체 문자를 데이터 행렬로 변환\n",
    "\n",
    "data = np.array([char2vec[char] for char in raw])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* input_size – The number of expected features in the input x\n",
    "* hidden_size – The number of features in the hidden state h\n",
    "* num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "* nonlinearity – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’\n",
    "* bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "* batch_first – If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "* dropout – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "* bidirectional – If True, becomes a bidirectional RNN. Default: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(CharRNN, self).__init__()        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "#         self.rnn = nn.GRU(input_size, hidden_size, num_layers)  # GRU는 nonlinearity 지원 안함.\n",
    "#         self.rnn = nn.RNN(input_size, hidden_size, num_layers, dropout=0.5)\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        out, hidden = self.rnn(input.view(1,1,-1), hidden)  # 1*1*67\n",
    "        out = self.fc(out.view(1,-1))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.num_layers, 1, self.hidden_size)).cuda() # weight 초기화\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size, hidden_size, output_size, num_layers\n",
    "\n",
    "model = CharRNN(data.shape[1], 500, data.shape[1], 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1627\n",
      "Epoch [1/5], Iter [100/1627] Loss: 326.3845\n",
      "1627\n",
      "Epoch [2/5], Iter [100/1627] Loss: 223.6533\n",
      "1627\n",
      "Epoch [3/5], Iter [100/1627] Loss: 186.1924\n",
      "1627\n",
      "Epoch [4/5], Iter [100/1627] Loss: 232.0170\n",
      "1627\n",
      "Epoch [5/5], Iter [100/1627] Loss: 167.7863\n"
     ]
    }
   ],
   "source": [
    "step = 100\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # starting point\n",
    "    sp = list(range(0, len(data) - 2 * step, step))\n",
    "    sp = np.add(sp, random.randint(0, step))\n",
    "    random.shuffle(sp)\n",
    "   \n",
    "    print(len(sp))\n",
    "    \n",
    "    for i in range(len(sp)) :\n",
    "    \n",
    "        hidden = model.init_hidden()\n",
    "        cost = 0\n",
    "\n",
    "        for pos in range(sp[i], sp[i] + step):\n",
    "            X = Variable(torch.from_numpy(data[pos]).type(torch.FloatTensor)).cuda()\n",
    "            y = torch.from_numpy(data[pos+1]).cuda()\n",
    "            \n",
    "            _, y = y.max(dim=0)\n",
    "            y = y.unsqueeze(0)\n",
    "            \n",
    "            pred, hidden = model(X, hidden)\n",
    "            \n",
    "            cost += loss(pred, Variable(y).cuda())\n",
    "\n",
    "        cost.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)  # explosion 방지.\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) == len(sp):\n",
    "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'%(epoch+1, num_epochs, i+1, len(sp), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Generated Text : \n",
      " That in what is the fore in to the King on a Poltendinge\n",
      "\n",
      "   Ham. A sight and mence this wat has whan in when with as the ferriner and\n",
      "\n",
      "   Polon. There holdon that the heare of tise wall seefe:\n",
      "Anght and stone\n",
      "\n",
      "   Hor. I my Lort\n",
      "\n",
      "   Hor. There shat with has, wath his do a Prechiosens. But bout not mige thos of in mone same of your who eat ingre sore werce,\n",
      "I know mide in heaue the fantring of on of in hend\n",
      "To his not, where'd and serale: who han heere\n",
      "A there windent of my Laer? she thy fould war\n"
     ]
    }
   ],
   "source": [
    "start_num = 1\n",
    "text = index2char[start_num]\n",
    "\n",
    "model.eval()\n",
    "hidden = model.init_hidden()\n",
    "\n",
    "X_test = Variable(torch.from_numpy(data[start_num]).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "for i in range(500) :\n",
    "\n",
    "    pre, hidden = model(X_test, hidden)\n",
    "\n",
    "    temp = pre.cpu().data.numpy()[0]  # 확률\n",
    "\n",
    "    best_5 = np.argsort(temp)[::-1][:5]\n",
    "    \n",
    "    # softmax\n",
    "    temp = np.exp(temp[best_5])\n",
    "    temp = temp / temp.sum()\n",
    "    \n",
    "    pre = np.random.choice(best_5, 1, p = temp)[0]\n",
    "    \n",
    "    curr_char = index2char[pre]\n",
    "    \n",
    "    text += curr_char\n",
    "    \n",
    "    # 다음 y 입력\n",
    "    X_test = Variable(torch.from_numpy(char2vec[curr_char]).type(torch.FloatTensor)).cuda()\n",
    "    \n",
    "print(\"* Generated Text : \\n\", text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
