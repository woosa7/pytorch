{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 02. Variable and Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation 을 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable containing --> 변할 수 있는 값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Variable(x, requires_grad=True)   # requires_grad=True : gradient 계산\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data  # --> 변하지 않는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 5.],\n",
       "        [7., 9.]], grad_fn=<AddBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 2*y + 1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z : tensor([[3., 5.],\n",
      "        [7., 9.]], grad_fn=<AddBackward>)\n",
      "y.requires_grad : True\n",
      "z.requires_grad : True\n",
      "\n",
      "y.grad : None\n",
      "z.grad : None\n",
      "\n",
      "y.grad_fn : None\n",
      "z.grad_fn : <AddBackward object at 0x0000024119DD07B8>\n"
     ]
    }
   ],
   "source": [
    "print(\"z :\", z)\n",
    "print(\"y.requires_grad :\", y.requires_grad)\n",
    "print(\"z.requires_grad :\", z.requires_grad)  # z는 y의 함수이므로 y가 변할 때 z도 함께 변한다.\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"y.grad :\", y.grad) # 아직 변화된 적이 없기 때문에 None 출력됨.\n",
    "print(\"z.grad :\", z.grad)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"y.grad_fn :\", y.grad_fn)\n",
    "print(\"z.grad_fn :\", z.grad_fn)  # z = 2*y + 1 이라는 function을 가지고 있기 때문."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = z.sum()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.requires_grad : True\n",
      "z.requires_grad : True\n",
      "out.requires_grad : True\n",
      "\n",
      "y.grad : None\n",
      "z.grad : None\n",
      "out.grad : None\n",
      "\n",
      "y.grad_fn : None\n",
      "z.grad_fn : <AddBackward object at 0x0000024119DCBC88>\n",
      "out.grad_fn : <SumBackward0 object at 0x0000024119DCBDD8>\n"
     ]
    }
   ],
   "source": [
    "print(\"y.requires_grad :\", y.requires_grad)\n",
    "print(\"z.requires_grad :\", z.requires_grad)\n",
    "print(\"out.requires_grad :\", out.requires_grad)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"y.grad :\", y.grad)\n",
    "print(\"z.grad :\", z.grad)\n",
    "print(\"out.grad :\", out.grad)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"y.grad_fn :\", y.grad_fn)\n",
    "print(\"z.grad_fn :\", z.grad_fn)\n",
    "print(\"out.grad_fn :\", out.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x24119dbac88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, gradients are only retained for leaf variables. --> 첫번째 선언한 variables만 유지됨.\n",
    "# non-leaf variables gradients are not retained. \n",
    "# This was done by design, to save memory.\n",
    "\n",
    "# non-leaf variables을 보기 위해서 새로운 변수 및 함수 선언.\n",
    "zGrad = torch.zeros(2,2)\n",
    "\n",
    "def extract(z):\n",
    "    global zGrad\n",
    "    zGrad = z\n",
    "    \n",
    "z.register_hook(extract) # 이 순간의 값을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# out으로부터 back propagation을 시작한다.... !!!\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.requires_grad : True\n",
      "z.requires_grad : True\n",
      "out.requires_grad : True\n",
      "\n",
      "y.grad : tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "z.grad : None\n",
      "out.grad : None\n",
      "\n",
      "y.grad_fn : None\n",
      "z.grad_fn : <AddBackward object at 0x0000024119DCE630>\n",
      "out.grad_fn : <SumBackward0 object at 0x0000024119DCE780>\n",
      "\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"y.requires_grad :\", y.requires_grad)\n",
    "print(\"z.requires_grad :\", z.requires_grad)\n",
    "print(\"out.requires_grad :\", out.requires_grad)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"y.grad :\", y.grad)      # y가 1 증가하면 grad는 2 증가. out.backward() 통한 역전파 값.\n",
    "print(\"z.grad :\", z.grad)      # non-leaf variables 이라서 표시 안됨.\n",
    "print(\"out.grad :\", out.grad)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"y.grad_fn :\", y.grad_fn)\n",
    "print(\"z.grad_fn :\", z.grad_fn)\n",
    "print(\"out.grad_fn :\", out.grad_fn)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(zGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
